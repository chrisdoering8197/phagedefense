{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native Locations\n",
    "#### This notebook searches the local genomic context of the experimentally discovered defense systems for the specific strains that they were discovered in (referred here as \"native location\"). Data from this notebook used to create figure S4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Packages and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages used in this notebook\n",
    "from Bio import SeqIO\n",
    "import re, math\n",
    "import random\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from Bio import SearchIO\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "OUT_FOLDER = '/home/cdoering/ChrisSysInContext/NativeLocations/'\n",
    "project_folder = '/home/cdoering/ChrisSysInContext/' \n",
    "DIdb = project_folder+\"DefenseDomains.hmm\" #HMM database of defense domains\n",
    "VOGdb = \"/home/cdoering/allVOG.hmm\" #HMM database of all pVOG domains\n",
    "DIsignFile = project_folder+'DISign.txt' #File denoting if a given domain in DIdb is either \"positive\" (defense-related) or \"negative\" (housekeeping-related)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Local Region Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sysLocs = pd.read_csv('DefSysNativeLocations.txt',sep = '\\t',names = ['System','Accession','ProtID'])\n",
    "sysLocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MultiGeneSys = ['T4_12','T4_28','T4_RT06','T4_RT11','Lambda_36','Lambda_37','Lambda_49','Lambda_51','T7_2','T7_5']\n",
    "SingleGeneSys = ['T4_11','T4_16','T4_34','T4_38','T4_43','T4_58','Lambda_11','Lambda_64','T7_38','T7_74','T7_43']\n",
    "sys2IDs = dict()\n",
    "for sys in MultiGeneSys:\n",
    "    sys2IDs[sys] = sysLocs[sysLocs['System'].str.contains(sys)]['ProtID'].tolist()\n",
    "for sys in SingleGeneSys:\n",
    "    sys2IDs[sys] = sysLocs[sysLocs['System'] == sys]['ProtID'].tolist()\n",
    "parts2Sys = dict()\n",
    "for part in sysLocs['System'].tolist():\n",
    "    if any([name in part for name in MultiGeneSys]):\n",
    "        parts2Sys[part] = [name for name in MultiGeneSys if name in part][0]\n",
    "    else:\n",
    "        parts2Sys[part] = part\n",
    "sys2IDs, parts2Sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genbank = pd.read_csv('/home/cdoering/assembly_summary.txt',sep = '\\t',skiprows = 1,usecols = ['# assembly_accession','taxid','species_taxid'])\n",
    "refseq = pd.read_csv('/home/cdoering/assembly_summary_refseq.txt',sep = '\\t',skiprows = 1,usecols = ['# assembly_accession','taxid','species_taxid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Double check presence of native location in database\n",
    "genbankAcc = genbank['# assembly_accession'].tolist()\n",
    "refseqAcc = refseq['# assembly_accession'].tolist()\n",
    "for index, row in sysLocs.iterrows():\n",
    "    if (row['Accession'] in genbankAcc) or (row['Accession'] in refseqAcc):\n",
    "        print(row['System']+' present in DB')\n",
    "    else:\n",
    "        print(row['System']+' absent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genbankLinkList = pd.read_csv('/home/cdoering/assembly_summary_full_genomes.txt',sep = '\\t',header = None,squeeze = True).tolist()\n",
    "refseqLinkList = pd.read_csv('/home/cdoering/assembly_summary_refseq_links.txt',sep = '\\t',header = None,squeeze = True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to download a fasta file from the NCBI ftp page.\n",
    "#Inputs:\n",
    "    #genome_id - Accession and strain ID for a given bacterial genome\n",
    "    #genbankLinkList - txt file of all Genbank full bacterial genomes ftp site links\n",
    "    #refseqLinkList - txt file of all Refseq bacterial genomes ftp site links\n",
    "#Output: None, function will download the protein fasta file for a genome (if it exists) from the NCBI ftp site and then unzip the file.\n",
    "def DownloadFasta(genome_id,genbankLinkList,refseqLinkList):\n",
    "    for link in genbankLinkList:\n",
    "        if genome_id in link:\n",
    "            command = ['wget','-P','/mnt/disks/storage/ncbi-genomes-2021-04-29/',link+'/*_protein.faa.gz']\n",
    "            subprocess.run(command)\n",
    "            protFile = glob.glob('/mnt/disks/storage/ncbi-genomes-2021-04-29/*'+genome_id+'*_protein.faa.gz')\n",
    "            if protFile:\n",
    "                protFile = protFile[0]\n",
    "                command = ['gzip','-d',protFile]\n",
    "                subprocess.run(command)\n",
    "                return\n",
    "    for link in refseqLinkList:\n",
    "        if genome_id in link:\n",
    "            command = ['wget','-P','/mnt/disks/storage/ncbi-genomes-2021-04-29/',link+'/*_protein.faa.gz']\n",
    "            subprocess.run(command)\n",
    "            protFile = glob.glob('/mnt/disks/storage/ncbi-genomes-2021-04-29/*'+genome_id+'*_protein.faa.gz')\n",
    "            if protFile:\n",
    "                protFile = protFile[0]\n",
    "                command = ['gzip','-d',protFile]\n",
    "                subprocess.run(command)\n",
    "                return\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFasta(file,protIDs,sysName):    \n",
    "    genome_idwPath = ('_').join(file.split('_')[:-2]) #remove _feature_table.txt from file name\n",
    "    genome_id = os.path.basename(genome_idwPath) #isolate accession and strain ID for genome\n",
    "    protFile = genome_idwPath+'_protein.faa' #Makes fasta file name\n",
    "    if not os.path.isfile(protFile):\n",
    "        print('No fasta file, attempting download...')\n",
    "        DownloadFasta(genome_id,genbankLinkList,refseqLinkList)\n",
    "        if not os.path.isfile(protFile): #Check again for successful download and if not pass and continue\n",
    "            print('Download failed. Continuing...')\n",
    "            return\n",
    "    #Read in feature table and identify any homologs present in this feature table        \n",
    "    FeatTable = pd.read_csv(file,sep = '\\t')\n",
    "    FeatTable = FeatTable[FeatTable['# feature'] == 'CDS']\n",
    "    prots = FeatTable[(FeatTable['product_accession'].isin(protIDs)) | (FeatTable['non-redundant_refseq'].isin(protIDs))]\n",
    "    aroundIDs = dict()\n",
    "    #If homologs are present, grap 10kb on either side of the homologs and save down into a file.\n",
    "    for index, row in prots.iterrows():\n",
    "        \n",
    "        if str(row['chromosome']) == 'nan':\n",
    "            contig = row['genomic_accession']\n",
    "            up10 = row['start'] - 10000\n",
    "            down10 = row['end'] + 10000\n",
    "            aroundProt = FeatTable[(FeatTable['start'] > up10) & \n",
    "                                         (FeatTable['end'] < down10) &\n",
    "                                        (FeatTable['genomic_accession'] == contig)]\n",
    "        else:\n",
    "            chromosome = row['chromosome']\n",
    "            contig = row['genomic_accession']\n",
    "            up10 = row['start'] - 10000\n",
    "            down10 = row['end'] + 10000\n",
    "            aroundProt = FeatTable[(FeatTable['start'] > up10) & \n",
    "                                         (FeatTable['end'] < down10) &\n",
    "                                        (FeatTable['chromosome'] == chromosome) &\n",
    "                                        (FeatTable['genomic_accession'] == contig)]\n",
    "        if row['product_accession'] in protIDs:\n",
    "            aroundIDs[row['product_accession']] = [x for x in aroundProt['product_accession'].values.tolist() if str(x) != 'nan']\n",
    "        elif row['non-redundant_refseq'] in protIDs:\n",
    "            aroundIDs[row['non-redundant_refseq']] = [x for x in aroundProt['product_accession'].values.tolist() if str(x) != 'nan']\n",
    "    for homolog in aroundIDs:\n",
    "        #OUT = homologs2main[homolog]+'/'+genome_id+'_'+homolog+'.faa'\n",
    "        OUT = OUT_FOLDER+sysName+'_'+genome_id+'_'+homolog+'.faa'\n",
    "        if os.path.isfile(OUT):\n",
    "            return\n",
    "        prots2grab = aroundIDs[homolog]+[homolog]\n",
    "        protRecords = []\n",
    "        for record in SeqIO.parse(protFile,'fasta'):\n",
    "            if record.id in prots2grab:\n",
    "                protRecords.append(record)\n",
    "        SeqIO.write(protRecords,OUT,'fasta')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in sysLocs.iterrows():\n",
    "    sysName = row['System']\n",
    "    acc = row['Accession']\n",
    "    protIDs = set([row['ProtID']])\n",
    "    if protIDs == 'No_ID(pseudo)':\n",
    "        continue\n",
    "    file = glob.glob('/mnt/disks/storage/ncbi-genomes-2021-04-29/*'+acc+'*_feature_table.txt')[0]\n",
    "    buildFasta(file,protIDs,sysName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defense and Housekeeping Domain Calculations\n",
    "def HMMERFiles(faFile):\n",
    "    outFile = os.path.splitext(faFile)[0]+'_hmmer.txt'\n",
    "    if os.path.isfile(outFile):\n",
    "        return\n",
    "    else:\n",
    "#         print('Starting HMMsearch')\n",
    "#         command = ['hmmsearch','-E',EVAL,'--tblout',outFile,DIdb,faFile]\n",
    "        print('Starting Hmmscan')\n",
    "        EVAL = '0.00001'\n",
    "        command = ['hmmscan','-E',EVAL,'--tblout',outFile,DIdb,faFile]\n",
    "        subprocess.run(command)\n",
    "        return\n",
    "#pVOG Calculations with Lower Evalue\n",
    "def HMMERFiles_VOG(faFile):\n",
    "    outFile = os.path.splitext(faFile)[0]+'_VOG_hmmer.txt'\n",
    "    if os.path.isfile(outFile):\n",
    "        return\n",
    "    else:\n",
    "        print('Starting HMMscan')\n",
    "        EVAL = '0.000000000000001'\n",
    "        command = ['hmmscan','-E',EVAL,'--tblout',outFile,VOGdb,faFile]\n",
    "        subprocess.run(command)\n",
    "        return\n",
    "#Searches HMMER file to find out if it was a hmmscan or hmmsearch run and return result as a string\n",
    "def hmmFileType(fileName):\n",
    "    searchType = None\n",
    "    with open(fileName,'r') as F:\n",
    "        for line in F:\n",
    "            if line.startswith('# Program:'):\n",
    "                if 'hmmscan' in line:\n",
    "                    searchType = 'scan'\n",
    "                if 'hmmsearch' in line:\n",
    "                    searchType = 'search'\n",
    "    if searchType == None:\n",
    "        raise ValueError('HMMER file type was not found')\n",
    "    return searchType\n",
    "\n",
    "#Function to extract domains from a given hmmsearch or hmmscan result tblout output\n",
    "#Input: filepath to a hmmscan or hmmsearch tblout file\n",
    "#Output: a dictionary of where every key is a protein accession number and the results are a list of all domain hits\n",
    "def HMMERhit_lister(filePath,searchType = 'scan'):\n",
    "    #HMMER files were generated using both hmmscan and hmmsearch functions which have slightly different output styles\n",
    "    #Note: hmmscan runs much faster for our purposes. Hmmsearch was used at first when I did not know this.\n",
    "    if searchType == 'scan':\n",
    "        result = pd.read_csv(filePath, sep = ' ', comment = '#',header = None,skipinitialspace = True,usecols = [0,1,2],\n",
    "                        names = ['Domain','DomainAcc','Query'])\n",
    "    if searchType == 'search':\n",
    "        result = pd.read_csv(filePath,sep = ' ',usecols = [0,2,3],skipinitialspace = True,header = None,comment = '#',\n",
    "                         names = ['Query','Domain','DomainAcc'])\n",
    "    resultDict = {}\n",
    "    for index, row in result.iterrows():\n",
    "        #Do to differences in the formatting of the COG and pVOG vs PFAM databases the domain name ... \n",
    "        #(and not a descriptive name) is stored in a different location (Domain vs DomainAcc for COG/pVOG vs PFAM)\n",
    "        if row.Domain.startswith('COG') or row.Domain.startswith('VOG'): \n",
    "            if row.Query not in resultDict:\n",
    "                resultDict[row.Query] = [row.Domain]\n",
    "            else:\n",
    "                resultDict[row.Query].append(row.Domain)\n",
    "        elif row.DomainAcc.startswith('PF'):\n",
    "            pfam = row.DomainAcc.split('.')[0]\n",
    "            if row.Query not in resultDict:\n",
    "                resultDict[row.Query] = [pfam]\n",
    "            else:\n",
    "                resultDict[row.Query].append(pfam)\n",
    "    return resultDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in positive and negative association of defense island related domains from file\n",
    "DISign = DIsignFile\n",
    "posDI = set()\n",
    "negDI = set()\n",
    "with open(DISign) as f:\n",
    "    for line in f:\n",
    "        (domain, sign) = line.split()\n",
    "        if sign == \"negative\":\n",
    "            negDI.add(domain)\n",
    "        elif sign == \"positive\":\n",
    "            posDI.add(domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "faFiles = glob.glob(OUT_FOLDER+'*.faa')\n",
    "for file in faFiles:\n",
    "    HMMERFiles(file)\n",
    "    HMMERFiles_VOG(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiGeneSys = ['T4_12','T4_28','T4_RT06','T4_RT11','Lambda_36','Lambda_37','Lambda_49','Lambda_51','T7_2','T7_5']\n",
    "\n",
    "regionSummary = pd.DataFrame(sysLocs['System'].tolist(),columns = ['System'])\n",
    "regionSummary['Num. with Defense'] = 0\n",
    "regionSummary['Num. Phage Associated'] = 0\n",
    "regionSummary['Total Proteins in Region'] = 0\n",
    "regionSummary['Defense Hits'] = np.empty((len(regionSummary), 0)).tolist()\n",
    "regionSummary['pVOG Hits'] = np.empty((len(regionSummary), 0)).tolist()\n",
    "\n",
    "OUT = OUT_FOLDER+'NativeLocationsSummaries.txt'\n",
    "for index, row in regionSummary.iterrows():\n",
    "    sysName = parts2Sys[row['System']]\n",
    "        \n",
    "    hmmerResults = glob.glob(OUT_FOLDER+sysName+'*_hmmer.txt')\n",
    "    DIResults = [x for x in hmmerResults if '_VOG_hmmer.txt' not in x][0]\n",
    "    \n",
    "    \n",
    "    originalIDs = sys2IDs[sysName]\n",
    "    \n",
    "    protAcc_wExt = os.path.basename(DIResults)\n",
    "    protAcc = ('_').join(protAcc_wExt.split('_')[:-1])\n",
    "\n",
    "    faFile = OUT_FOLDER+protAcc+'.faa'\n",
    "    diFile = OUT_FOLDER+protAcc+'_hmmer.txt'\n",
    "    vogFile = OUT_FOLDER+protAcc+'_VOG_hmmer.txt'\n",
    "\n",
    "    #Read hmmer results into python\n",
    "    DIsearchType = hmmFileType(diFile)\n",
    "    DIhitDict = HMMERhit_lister(diFile,DIsearchType)\n",
    "\n",
    "    VOGsearchType = hmmFileType(vogFile)\n",
    "    VOGhitDict = HMMERhit_lister(vogFile,VOGsearchType)\n",
    "\n",
    "    for record in SeqIO.parse(faFile,'fasta'):\n",
    "        regionSummary.at[index,'Total Proteins in Region'] += 1\n",
    "\n",
    "        if (record.id in DIhitDict) & (record.id not in originalIDs):\n",
    "            DIdomains = DIhitDict[record.id]\n",
    "            if any([dom in posDI for dom in DIdomains]):\n",
    "                regionSummary.at[index,'Num. with Defense'] += 1\n",
    "                regionSummary.at[index,'Defense Hits'].append((record.id,[dom for dom in DIdomains if dom in posDI]))\n",
    "        if (record.id in VOGhitDict) & (record.id not in originalIDs):\n",
    "            VOGdomains = VOGhitDict[record.id]\n",
    "            if any([dom.startswith('VOG') for dom in VOGdomains]):\n",
    "                regionSummary.at[index,'Num. Phage Associated'] += 1\n",
    "                regionSummary.at[index,'pVOG Hits'].append((record.id,[dom for dom in VOGdomains if dom.startswith('VOG')]))\n",
    "\n",
    "    regionSummary.to_csv(OUT,sep = '\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
