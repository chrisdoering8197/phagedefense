{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster-Based Scoring Method\n",
    "#### This notebook takes in lists of defense systems with genomic contexts (all CDS +/- 10 kB of system) as generated by SysinContext_Final.ipynb and looks at the enrichment of defense domain and phage domain containing proteins in the nearby proteins. Data from this notebook used to make Figure S6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Packages, Files, and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages used in this notebook\n",
    "from Bio import SeqIO\n",
    "import re, math\n",
    "import random\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from Bio import SearchIO\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "project_folder = '/home/cdoering/ChrisSysInContext/' #Folder where this notebook is stored and where outputs are stored\n",
    "DIdb = project_folder+\"DefenseDomains.hmm\" #HMM database of defense domains\n",
    "VOGdb = \"/home/cdoering/allVOG.hmm\" #HMM database of all pVOG domains\n",
    "DIsignFile = project_folder+'DISign.txt' #File denoting if a given domain in DIdb is either \"positive\" (defense-related) or \"negative\" (housekeeping-related)\n",
    "OUT_FOLDER = project_folder+'ClusterScore/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in positive and negative association of defense island related domains from file\n",
    "DISign = DIsignFile\n",
    "posDI = set()\n",
    "negDI = set()\n",
    "with open(DISign) as f:\n",
    "    for line in f:\n",
    "        (domain, sign) = line.split()\n",
    "        if sign == \"negative\":\n",
    "            negDI.add(domain)\n",
    "        elif sign == \"positive\":\n",
    "            posDI.add(domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to make a list of all accession numbers for a given original system name\n",
    "def fetchHomologList(original_idwPath):\n",
    "    OGFastaList = glob.glob(original_idwPath+\"_blastHomologs\")[0] #get blast output file\n",
    "    IDs = pd.read_csv(OGFastaList,delim_whitespace = True,usecols=[0],header=None,squeeze = True).tolist() #grab accession numbers\n",
    "    IDs = [x.split(\"|\")[1] for x in IDs] #drop genbank or refseq demarcation and keep only the accession number\n",
    "    return IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fasta file building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHRIS_IN = project_folder+'Cov80Data/'\n",
    "OTHER_IN = project_folder+'SorekandZhang/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMultiOverlaps(componentName):\n",
    "    Overlap = set()\n",
    "    sysName = ('-').join(componentName.split('-')[:-1])\n",
    "    ChrisSys = glob.glob(CHRIS_IN+sysName+'*/')\n",
    "    OtherSys = glob.glob(OTHER_IN+sysName+'*/')\n",
    "    if ChrisSys:\n",
    "        multiFolders = ChrisSys\n",
    "    if OtherSys:\n",
    "        multiFolders = OtherSys\n",
    "\n",
    "    AccSets = [None]*len(multiFolders)\n",
    "    for i in range(len(multiFolders)):\n",
    "        allAccs = glob.glob(multiFolders[i][:-1]+'_blastHomologs')[0]\n",
    "        allAccs = pd.read_csv(allAccs,delim_whitespace = True,usecols=[0],header=None,squeeze = True).tolist() #read in all IDs\n",
    "        allAccs = [Acc.split(\"|\")[1] for Acc in allAccs] #remove genbank or refseq demarkation attached to accession by | mark\n",
    "        AccSets[i] = set(allAccs)\n",
    "        \n",
    "    folderofInt = [fold for fold in multiFolders if componentName in fold][0]\n",
    "    FAAs_0 = glob.glob(folderofInt+'*.faa')\n",
    "    for FAA in FAAs_0:\n",
    "        allPresent = [False]*len(AccSets)\n",
    "        for record in SeqIO.parse(FAA,'fasta'):\n",
    "            for i in range(len(AccSets)):\n",
    "                if record.id in AccSets[i]:\n",
    "                    allPresent[i] = True\n",
    "        if all(allPresent):\n",
    "            Overlap = Overlap.union(set([FAA]))\n",
    "\n",
    "    return Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Compile all non-system fasta sequences into a single file for each system.\n",
    "MultiGeneSys = ['Zorya1','Zorya2','Kiwa','Durantia','DRT1','DRT3','RADAR1','RADAR2','AVAST1','AVAST3','SIR2_HerA'\n",
    "                ,'DUF4297_HerA','qatABCD','mzaABCDE','TerY_P','ietAS','RM_like','Theoris1','Theoris2','Hachiman'\n",
    "                ,'Gabija1','Gabija2','Septu1','Septu2','Lamassu','Wadjet1','Wadjet2','Wadjet3'\n",
    "                ,'Lambda_37','Lambda_49','T4_12','T4_28','T7_5','Lambda_36','Lambda_51','T4_RT11']\n",
    "\n",
    "allFolders = glob.glob(CHRIS_IN+'*/') + glob.glob(OTHER_IN+'*/') + glob.glob(RANDOM_IN+'*/')\n",
    "for folder in allFolders:\n",
    "    \n",
    "    componentName = folder.split('/')[-2]\n",
    "    OUT_FILE = OUT_FOLDER+componentName+'_neighbors.faa'\n",
    "    if os.path.isfile(OUT_FILE):\n",
    "        continue\n",
    "    \n",
    "    sysName = componentName.split('-')[0]\n",
    "    syswPath = folder[:-1]\n",
    "    if sysName in MultiGeneSys:\n",
    "        print('System: '+sysName+' component '+componentName)\n",
    "        multiFolders = glob.glob(('-').join(folder.split('-')[:-1])+'*/')\n",
    "        originalIDs = []\n",
    "        for sysPart in multiFolders:\n",
    "            originalIDs = originalIDs + fetchHomologList(sysPart[:-1])\n",
    "            \n",
    "        allFasta = list(getMultiOverlaps(componentName))\n",
    "    else:\n",
    "        print('System: '+sysName)\n",
    "        originalIDs = fetchHomologList(syswPath)\n",
    "        \n",
    "        allFasta = glob.glob(folder+'*.faa')\n",
    "\n",
    "    sequences = []\n",
    "    for faa in allFasta:\n",
    "        for record in SeqIO.parse(faa,'fasta'):\n",
    "            if record.id not in originalIDs:\n",
    "                sequences.append(record)\n",
    "    SeqIO.write(sequences,OUT_FILE,'fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Domain Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run hmmscan on Sorek and Zhang neighbors for defense\n",
    "def HMMERFiles(faFile):\n",
    "    outFile = os.path.splitext(faFile)[0]+'_hmmer.txt'\n",
    "    if os.path.isfile(outFile):\n",
    "        return\n",
    "    else:\n",
    "        EVAL = '0.00001'\n",
    "        print('Starting Hmmscan')\n",
    "        command = ['hmmscan','-E',EVAL,'--tblout',outFile,DIdb,faFile]\n",
    "        subprocess.run(command)\n",
    "        return\n",
    "if __name__ == '__main__':\n",
    "    SZFiles = [] #TODO\n",
    "    FAAs = glob.glob(OUT_FOLDER+'*_neighbors.faa')\n",
    "    for FAA in FAAs:\n",
    "        if any([sys in FAA for sys in SZComponents]):\n",
    "            SZFiles = SZFiles + [FAA]\n",
    "    pool = Pool()\n",
    "    pool.map(HMMERFiles,FAAs)\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run hmmscan on Sorek and Zhang neighbors for pVOGs\n",
    "def HMMERFiles_VOG(faFile):\n",
    "    outFile = os.path.splitext(faFile)[0]+'_VOG_hmmer.txt'\n",
    "    if os.path.isfile(outFile):\n",
    "        return\n",
    "    else:\n",
    "        print('Starting HMMscan')\n",
    "        EVAL = '0.000000000000001'\n",
    "        command = ['hmmscan','-E',EVAL,'--tblout',outFile,VOGdb,faFile]\n",
    "        subprocess.run(command)\n",
    "        return\n",
    "if __name__ == '__main__':\n",
    "    SZFiles = [] #TODO\n",
    "    FAAs = glob.glob(OUT_FOLDER+'*_neighbors.faa')\n",
    "    for FAA in FAAs:\n",
    "        if any([sys in FAA for sys in SZComponents]):\n",
    "            SZFiles = SZFiles + [FAA]\n",
    "    pool = Pool()\n",
    "    pool.map(HMMERFiles_VOG,FAAs)\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster neighboring proteins\n",
    "allFaas = glob.glob(OUT_FOLDER+'*_neighbors.faa')\n",
    "for faaFile in allFaas:\n",
    "    \n",
    "    if os.listdir(OUT_FOLDER+'tmp/'):\n",
    "        command = ['rm','-r',OUT_FOLDER+'tmp']\n",
    "        subprocess.run(command)\n",
    "        os.mkdir(OUT_FOLDER+'tmp/')\n",
    "        \n",
    "    seqDB = os.path.splitext(faaFile)[0]+'DB'\n",
    "    clustDB = os.path.splitext(faaFile)[0]+'Clust'\n",
    "    tsvFile = clustDB+'.tsv'\n",
    "    clustseqDB = clustDB+'Seq'\n",
    "    clustFasta = os.path.splitext(faaFile)[0]+'Clust.faa'\n",
    "    \n",
    "    if os.path.isfile(clustFasta):\n",
    "        continue\n",
    "        \n",
    "    command = ['mmseqs','createdb',faaFile,seqDB]\n",
    "    subprocess.run(command)\n",
    "    command = ['mmseqs','cluster',seqDB,clustDB,OUT_FOLDER+'tmp/','--min-seq-id','0.9','--cluster-mode','1']\n",
    "    subprocess.run(command)\n",
    "    command = ['mmseqs','createtsv',seqDB,clustDB,tsvFile]\n",
    "    subprocess.run(command)\n",
    "    command = ['mmseqs','createseqfiledb',seqDB,clustDB,clustseqDB]\n",
    "    subprocess.run(command)\n",
    "    command = ['mmseqs','result2flat',seqDB,seqDB,clustseqDB,clustFasta]\n",
    "    subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cluster Domain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract domains from a given hmmsearch or hmmscan result tblout output\n",
    "#Input: filepath to a hmmscan or hmmsearch tblout file\n",
    "#Output: a dictionary of where every key is a protein accession number and the results are a list of all domain hits\n",
    "def HMMERhit_lister(filePath,searchType = 'scan'):\n",
    "    #HMMER files were generated using both hmmscan and hmmsearch functions which have slightly different output styles\n",
    "    #Note: hmmscan runs much faster for our purposes. Hmmsearch was used at first when I did not know this.\n",
    "    if searchType == 'scan':\n",
    "        result = pd.read_csv(filePath, sep = ' ', comment = '#',header = None,skipinitialspace = True,usecols = [0,1,2],\n",
    "                        names = ['Domain','DomainAcc','Query'])\n",
    "    if searchType == 'search':\n",
    "        result = pd.read_csv(filePath,sep = ' ',usecols = [0,2,3],skipinitialspace = True,header = None,comment = '#',\n",
    "                         names = ['Query','Domain','DomainAcc'])\n",
    "    resultDict = {}\n",
    "    for index, row in result.iterrows():\n",
    "        #Do to differences in the formatting of the COG and pVOG vs PFAM databases the domain name ... \n",
    "        #(and not a descriptive name) is stored in a different location (Domain vs DomainAcc for COG/pVOG vs PFAM)\n",
    "        if row.Domain.startswith('COG') or row.Domain.startswith('VOG'): \n",
    "            if row.Query not in resultDict:\n",
    "                resultDict[row.Query] = [row.Domain]\n",
    "            else:\n",
    "                resultDict[row.Query].append(row.Domain)\n",
    "        elif row.DomainAcc.startswith('PF'):\n",
    "            pfam = row.DomainAcc.split('.')[0]\n",
    "            if row.Query not in resultDict:\n",
    "                resultDict[row.Query] = [pfam]\n",
    "            else:\n",
    "                resultDict[row.Query].append(pfam)\n",
    "    return resultDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searches HMMER file to find out if it was a hmmscan or hmmsearch run and return result as a string\n",
    "def hmmFileType(fileName):\n",
    "    searchType = None\n",
    "    with open(fileName,'r') as F:\n",
    "        for line in F:\n",
    "            if line.startswith('# Program:'):\n",
    "                if 'hmmscan' in line:\n",
    "                    searchType = 'scan'\n",
    "                if 'hmmsearch' in line:\n",
    "                    searchType = 'search'\n",
    "    if searchType == None:\n",
    "        raise ValueError('HMMER file type was not found')\n",
    "    return searchType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append results from two HMMER hit dictionaries so that each protein ID has a single list of all domains associated with it\n",
    "def HMMERhit_append(dict1,dict2):\n",
    "    for key in dict2:\n",
    "        if key in dict1:\n",
    "            dict1[key] = dict1[key] + dict2[key]\n",
    "        else:\n",
    "            dict1[key] = dict2[key]\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allClusts = glob.glob(OUT_FOLDER+'*_neighborsClust.faa')\n",
    "allClustsDict = dict()\n",
    "for clust in allClusts:\n",
    "    clustDict = dict()\n",
    "    componentName = os.path.basename(('_').join(clust.split('_')[:-1]))\n",
    "    sysName = componentName.split('-')[0]\n",
    "    \n",
    "    ChrisSys = glob.glob(CHRIS_IN+componentName+'/*hmmer.txt')\n",
    "    if ChrisSys:\n",
    "        HMMERfiles = ChrisSys\n",
    "    else:\n",
    "        HMMERfiles = glob.glob(OUT_FOLDER+componentName+'*hmmer.txt')\n",
    "    DIfiles = [x for x in HMMERfiles if 'VOG_hmmer.txt' not in x]\n",
    "    VOGfiles = [x for x in HMMERfiles if 'VOG_hmmer.txt' in x]\n",
    "\n",
    "    DIhits = HMMERhit_lister(DIfiles[0],hmmFileType(DIfiles[0]))\n",
    "    if len(DIfiles) > 1:\n",
    "        for i in range(1,len(DIfiles)):\n",
    "            DIhits = HMMERhit_append(DIhits,HMMERhit_lister(DIfiles[i],hmmFileType(DIfiles[i])))\n",
    "    VOGhits = HMMERhit_lister(VOGfiles[0],hmmFileType(VOGfiles[0]))\n",
    "    if len(VOGfiles) > 1:\n",
    "        for i in range(1,len(VOGfiles)):\n",
    "            VOGhits = HMMERhit_append(VOGhits,HMMERhit_lister(VOGfiles[i],hmmFileType(VOGfiles[i])))\n",
    "    \n",
    "    headers = [] #List of tuples with accession first, DI domains 2nd, pVOG domains 3rd\n",
    "    with open(clust) as F:\n",
    "        for line in F:\n",
    "            cleanline = line.strip().split(' ')[0]\n",
    "            if cleanline.startswith('>'):\n",
    "                if cleanline[1:] in DIhits:\n",
    "                    DI = DIhits[cleanline[1:]]\n",
    "                else:\n",
    "                    DI = []\n",
    "                if cleanline[1:] in VOGhits:\n",
    "                    VOG = VOGhits[cleanline[1:]]\n",
    "                else:\n",
    "                    VOG = []\n",
    "                headers.append((cleanline[1:],set(DI),set(VOG)))\n",
    "    currClust = None\n",
    "    for i in range(len(headers)-1):\n",
    "        if headers[i][0] == headers[i+1][0]:\n",
    "            currClust = headers[i][0]\n",
    "        elif currClust in clustDict:\n",
    "            clustDict[currClust].append(headers[i])\n",
    "        else:\n",
    "            clustDict[currClust] = [headers[i]]\n",
    "            \n",
    "    allClustsDict[componentName] = clustDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check for relatively homogenious clusters\n",
    "for comp in allClustsDict:\n",
    "    DISim = [False] * len(allClustsDict[comp])\n",
    "    VOGSim = [False] * len(allClustsDict[comp])\n",
    "    simThresh = 0.9\n",
    "    clustCount = 0\n",
    "    for clust in allClustsDict[comp]:\n",
    "        allDI = [x[1] for x in allClustsDict[comp][clust]]\n",
    "        DI_simFreq = allDI.count((max(allDI,key = allDI.count)))/len(allDI)\n",
    "        if DI_simFreq >= simThresh:\n",
    "            DISim[clustCount] = True\n",
    "        allVOG = [x[2] for x in allClustsDict[comp][clust]]\n",
    "        VOG_simFreq = allVOG.count((max(allVOG,key = allVOG.count)))/len(allVOG)\n",
    "        if VOG_simFreq >= simThresh:\n",
    "            VOGSim[clustCount] = True\n",
    "        clustCount += 1\n",
    "    print(comp+' #Clusts: '+str(len(DISim))+' DISim True: '+str(DISim.count(True))+' VOGSim True: '+str(VOGSim.count(True))+\n",
    "         ' %DISim True: ' + str(DISim.count(True)/len(DISim))+ ' %VogSim True:'+str(VOGSim.count(True)/len(DISim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clustScores = pd.DataFrame(list(allClustsDict.keys()),columns = ['Component'])\n",
    "clustScores['DI'] = 0\n",
    "clustScores['Unique DI'] = 0\n",
    "clustScores['VOG'] = 0\n",
    "clustScores['Unique VOG'] = 0\n",
    "clustScores['Total'] = 0\n",
    "for comp in allClustsDict:\n",
    "    DI = 0\n",
    "    uDI = set()\n",
    "    VOG = 0\n",
    "    uVOG = set()\n",
    "    total = len(allClustsDict[comp])\n",
    "    thresh = 0.9\n",
    "    for clust in allClustsDict[comp]:\n",
    "        allDI = [x[1] for x in allClustsDict[comp][clust]]\n",
    "        withDI = [x for x in allDI if x]\n",
    "        for x in withDI:\n",
    "            uDI.add(tuple(x))\n",
    "        if len(withDI)/len(allDI) >= thresh:\n",
    "            DI += 1\n",
    "        allVOG = [x[2] for x in allClustsDict[comp][clust]]\n",
    "        withVOG = [x for x in allVOG if x]\n",
    "        for x in withVOG:\n",
    "            uVOG.add(tuple(x))\n",
    "        if len(withVOG)/len(allVOG) >= thresh:\n",
    "            VOG += 1\n",
    "    clustScores.loc[clustScores.Component == comp,'DI'] = DI\n",
    "    clustScores.loc[clustScores.Component == comp,'Unique DI'] = len(uDI)\n",
    "    clustScores.loc[clustScores.Component == comp,'VOG'] = VOG\n",
    "    clustScores.loc[clustScores.Component == comp,'Unique VOG'] = len(uVOG)\n",
    "    clustScores.loc[clustScores.Component == comp,'Total'] = total\n",
    "clustScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustScores['DI Percent'] = clustScores['DI']/clustScores['Total']\n",
    "clustScores['VOG Percent'] = clustScores['VOG']/clustScores['Total']\n",
    "clustScores['Unique DI Percent'] = clustScores['Unique DI']/clustScores['DI']\n",
    "clustScores['Unique VOG Percent'] = clustScores['Unique VOG']/clustScores['VOG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustScores['Group'] = None\n",
    "for index, row in clustScores.iterrows():\n",
    "    if ('T4' in row['Component']) | ('T7' in row['Component']) | ('Lambda' in row['Component']):\n",
    "        clustScores.at[index,'Group'] = 'Chris'\n",
    "    else:\n",
    "        clustScores.at[index,'Group'] = 'SZ'\n",
    "clustScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MultiGeneSys = ['Zorya1','Zorya2','Kiwa','Durantia','DRT1','DRT3','RADAR1','RADAR2','AVAST1','AVAST3','SIR2_HerA'\n",
    "                ,'DUF4297_HerA','qatABCD','mzaABCDE','TerY_P','ietAS','RM_like','Theoris1','Theoris2','Hachiman'\n",
    "                ,'Gabija1','Gabija2','Septu1','Septu2','Lamassu','Wadjet1','Wadjet2','Wadjet3'\n",
    "                ,'Lambda_37','Lambda_49','T4_12','T4_28','T7_5','Lambda_36','Lambda_51','T4_RT11']\n",
    "for sys in MultiGeneSys:\n",
    "    sysParts = [x for x in clustScores.Component.tolist() if sys in x]\n",
    "    sysTotals = []\n",
    "    for part in sysParts:\n",
    "        sysTotals.append(clustScores[clustScores['Component'] == part]['Total'].tolist()[0])\n",
    "    maxPart = sysParts[sysTotals.index(max(sysTotals))]\n",
    "    for part in sysParts:\n",
    "        if part != maxPart:\n",
    "            clustScores.drop(clustScores.index[clustScores.Component == part],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustScores['Homologs'] = None\n",
    "for index, row in clustScores.iterrows():\n",
    "    comp = row['Component']\n",
    "    ChrisSys = glob.glob(CHRIS_IN+comp+'/*.faa')\n",
    "    SZSys = glob.glob(OTHER_IN+comp+'/*.faa')\n",
    "    RandomSys = glob.glob(RANDOM_IN+comp+'/*.faa')\n",
    "    homologs = []\n",
    "    if ChrisSys:\n",
    "        homologs = ChrisSys\n",
    "    if SZSys:\n",
    "        homologs = SZSys\n",
    "    if RandomSys:\n",
    "        homologs = RandomSys\n",
    "    clustScores.at[index,'Homologs'] = len(homologs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustScores.drop(clustScores.index[clustScores.Component == 'T4_RT06-B'],inplace = True)\n",
    "clustScores.drop(clustScores.index[clustScores.Component == 'T4_RT06-C'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustScores['Total Proteins'] = None\n",
    "for index, row in clustScores.iterrows():\n",
    "    comp = row['Component']\n",
    "    total = 0\n",
    "    for record in SeqIO.parse(OUT_FOLDER+comp+'_neighbors.faa','fasta'):\n",
    "        total += 1\n",
    "    clustScores.at[index,'Total Proteins'] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustScores['DI/TotalProteins'] = clustScores['DI']/clustScores['Total Proteins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustScores.to_csv('ClusterScores.txt',sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Graphing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(data=clustScores,x = 'Group',y='Homologs')\n",
    "sns.stripplot(data=clustScores,x = 'Group',y='Homologs')\n",
    "plt.ylabel('Total System Homologs')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=clustScores,x = 'Group',y='Total')\n",
    "sns.stripplot(data=clustScores,x = 'Group',y='Total')\n",
    "plt.ylabel('Total Clusters')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = clustScores[clustScores['Group'] == 'Chris']['DI/TotalProteins']\n",
    "SZ = clustScores[clustScores['Group'] == 'SZ']['DI/TotalProteins']\n",
    "stats.mannwhitneyu(C,SZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=clustScores,x = 'Group',y='DI/TotalProteins')\n",
    "sns.stripplot(data=clustScores,x = 'Group',y='DI/TotalProteins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=clustScores,x = 'DI',y = 'Unique DI',hue = 'Group')\n",
    "plt.xlabel('Defense Clusters')\n",
    "plt.ylabel('Unique Defense Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=clustScores,x = 'VOG',y = 'Unique VOG',hue = 'Group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=clustScores,x = 'Homologs',y = 'Unique DI Percent',hue = 'Group')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=clustScores,x = 'Group',y='DI')\n",
    "sns.stripplot(data=clustScores,x = 'Group',y='DI')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_DI = clustScores[clustScores['Group'] == 'Chris']['DI']\n",
    "SZ_DI = clustScores[clustScores['Group'] == 'SZ']['DI']\n",
    "stats.mannwhitneyu(C_DI,SZ_DI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=clustScores,x = 'Group',y='Unique DI Percent')\n",
    "sns.stripplot(data=clustScores,x = 'Group',y='Unique DI Percent')\n",
    "plt.ylabel('Percentage Unique Defense Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_DIUn = clustScores[clustScores['Group'] == 'Chris']['Unique DI']\n",
    "SZ_DIUn = clustScores[clustScores['Group'] == 'SZ']['Unique DI']\n",
    "stats.mannwhitneyu(C_DIUn,SZ_DIUn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=clustScores,x = 'Group',y='DI Percent')\n",
    "sns.stripplot(data=clustScores,x = 'Group',y='DI Percent')\n",
    "plt.ylabel('Defense Percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_DIPerc = clustScores[clustScores['Group'] == 'Chris']['DI Percent']\n",
    "SZ_DIPerc = clustScores[clustScores['Group'] == 'SZ']['DI Percent']\n",
    "stats.mannwhitneyu(C_DIPerc,SZ_DIPerc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=clustScores,x = 'Group',y='VOG Percent')\n",
    "sns.stripplot(data=clustScores,x = 'Group',y='VOG Percent')\n",
    "plt.ylabel('Phage Percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "C_VOGPerc = clustScores[clustScores['Group'] == 'Chris']['VOG Percent']\n",
    "SZ_VOGPerc = clustScores[clustScores['Group'] == 'SZ']['VOG Percent']\n",
    "stats.mannwhitneyu(C_VOGPerc,SZ_VOGPerc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
