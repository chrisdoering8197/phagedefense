{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomic Data for Test and Control Systems\n",
    "#### This notebook searches for all homologs of the experimentally discovered phage defense systems - and controls - in our genomic database. It then records the TaxID of each genome in which the systems are found and saves this information. Genome accessions for experimentally found systems are pulled from the files generated from the analysis in SysinContext_final.ipynbData from this notebook used to create Figure 3E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Needed packages and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import re, math\n",
    "import random\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from Bio import SearchIO\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "dbFolder = '/mnt/disks/storage/ncbi-genomes-2021-04-29/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to make a list of all accession numbers for a given original system name\n",
    "def fetchHomologList(original_id):\n",
    "    OGFastaList = glob.glob(OUT_FOLDER+original_id+\"_blastHomologs\")[0] #get blast output file\n",
    "    IDs = pd.read_csv(OGFastaList,delim_whitespace = True,usecols=[0],header=None,squeeze = True).tolist() #grab accession numbers\n",
    "    IDs = [x.split(\"|\")[1] for x in IDs] #drop genbank or refseq demarcation and keep only the accession number\n",
    "    return IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build table of all genomic accessions with their TaxID and Species TaxID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FTs = glob.glob(dbFolder+'*feature_table.txt')\n",
    "gAccessions = [None]*len(FTs)\n",
    "ftNum = 0\n",
    "for FT in FTs:\n",
    "    if ftNum % 1000 == 0:\n",
    "        print(str(ftNum)+' out of '+str(len(gAccessions)))\n",
    "    genomeAccession = pd.read_csv(FT,sep = '\\t',usecols = ['assembly'],squeeze = True).tolist()[0]\n",
    "    gAccessions[ftNum] = genomeAccession\n",
    "    ftNum += 1\n",
    "with open('/home/cdoering/ChrisSysInContext/ChrisDBAssemblyAccessions.txt','w') as file:\n",
    "    for accession in gAccessions:\n",
    "        file.write(accession+' \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gAccessions = pd.read_csv('/home/cdoering/ChrisSysInContext/ChrisDBAssemblyAccessions.txt',sep = '\\t',header = None,squeeze = True)\n",
    "gAccessions = gAccessions.str.strip().tolist()\n",
    "gAccessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assembly taxid and species taxid information for every genbank and refseq genome and store in dictionary\n",
    "genbank = pd.read_csv('/home/cdoering/assembly_summary.txt',sep = '\\t',skiprows = 1,usecols = ['# assembly_accession','taxid','species_taxid'])\n",
    "refseq = pd.read_csv('/home/cdoering/assembly_summary_refseq.txt',sep = '\\t',skiprows = 1,usecols = ['# assembly_accession','taxid','species_taxid'])\n",
    "\n",
    "genbankDict = {row[0]:(row[1],row[2]) for index, row in genbank.iterrows()}\n",
    "refseqDict = {row[0]:(row[1],row[2]) for index, row in refseq.iterrows()}\n",
    "Acc2Taxid = {**genbankDict, **refseqDict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Write to file the taxid for every genome in the dataset\n",
    "allDBTaxids.txt\n",
    "allTaxids = pd.DataFrame(gAccessions,columns = ['Accession'])\n",
    "allTaxids['Taxid'] = None\n",
    "allTaxids['Species Taxid'] = None\n",
    "for index, row in allTaxids.iterrows():\n",
    "    allTaxids.at[index,'Taxid'] = Acc2Taxid[row['Accession']][0]\n",
    "    allTaxids.at[index,'Species Taxid'] = Acc2Taxid[row['Accession']][1]\n",
    "allTaxids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTaxids.to_csv('Taxonomy/allDBTaxids.txt',sep = '\\t',index_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTaxids = pd.read_csv('Taxonomy/allDBTaxids.txt',sep = '\\t')\n",
    "allTaxids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TaxIDs for Experimental Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systems = glob.glob('/home/cdoering/ChrisSysInContext/Cov80Summaries/*')\n",
    "for sys in systems:\n",
    "    OUT = '/home/cdoering/ChrisSysInContext/Taxonomy/'+('_').join(os.path.basename(sys).split('_')[:-1])+'_TaxIDs'\n",
    "    homologs = pd.read_csv(sys,sep = '\\t',skiprows = 6,usecols = ['Accession'])\n",
    "    outDF = pd.DataFrame([None]*len(homologs),columns = ['Accession'])\n",
    "    outDF['Taxid'] = None\n",
    "    outDF['Species Taxid'] = None\n",
    "    for index, row in homologs.iterrows():\n",
    "        file = os.path.basename(row[0])\n",
    "        Acc = file.split('.')[0]\n",
    "        version = file.split('.')[1].split('_')[0]\n",
    "        Acc = Acc + '.' + version\n",
    "        outDF.at[index,'Accession'] = Acc\n",
    "        outDF.at[index,'Taxid'] = Acc2Taxid[Acc][0]\n",
    "        outDF.at[index,'Species Taxid'] = Acc2Taxid[Acc][1]\n",
    "    outDF.to_csv(OUT,sep = '\\t',index_label = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TaxIDs for Control systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_FOLDER = 'TaxonomyControls/' #Folder to output data to\n",
    "IN_FASTA = '20211208_ControlSystems.faa' #Input list of proteins to search for\n",
    "MultiGeneSys = ['RM_1','RM_2','RM_3','RM_4','Cas','Zorya_I','ZoryaII','Kiwa','Durantia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take in fasta file with all proteins and split into individual files, create folders to store each proteins results\n",
    "for record in SeqIO.parse(IN_FASTA,'fasta'):\n",
    "    SUB = OUT_FOLDER+record.id\n",
    "    if os.path.isdir(SUB) == False:\n",
    "        os.mkdir(SUB)\n",
    "    if os.path.isfile(OUT_FOLDER+record.id+'.faa') == False:\n",
    "        SeqIO.write(record,OUT_FOLDER+record.id+'.faa','fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLASTP for Multi-Gene Systems. No coverage requirement\n",
    "#Run blastp on all proteins against the non-redundant database to get protein accessions\n",
    "EVAL = '0.00001'\n",
    "num = 1\n",
    "for system in MultiGeneSys:\n",
    "    starters = glob.glob(OUT_FOLDER+system+\"*.faa\")\n",
    "    for init in starters:\n",
    "        print('Blasting #'+str(num)+', '+init)\n",
    "        num += 1\n",
    "        hitName = os.path.splitext(os.path.basename(init))[0]\n",
    "        if os.path.isfile(OUT_FOLDER+hitName+'_blastHomologs') == False:\n",
    "            command = ['blastp','-query',init,\n",
    "                       '-db','/mnt/disks/storage/nr/nr',\n",
    "                       '-evalue',EVAL,\n",
    "                       '-out',OUT_FOLDER+hitName+'_blastHomologs',\n",
    "                       '-outfmt','6 sseqid evalue length qlen qstart qend slen sstart send',\n",
    "                      '-max_target_seqs','10000000',\n",
    "                      '-num_threads','6',\n",
    "                      '-taxidlist','bacterial.ids']\n",
    "            subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#BLASTP for Single-Gene Systems\n",
    "#Run blastp on all proteins against the non-redundant database to get protein accessions\n",
    "EVAL = '0.00001'\n",
    "starters = glob.glob(OUT_FOLDER+\"*.faa\")\n",
    "num = 1\n",
    "for init in starters:\n",
    "    print('Blasting #'+str(num)+' out of '+str(len(starters)))\n",
    "    num += 1\n",
    "    hitName = os.path.splitext(os.path.basename(init))[0]\n",
    "    if os.path.isfile(OUT_FOLDER+hitName+'_blastHomologs') == False:\n",
    "        command = ['blastp','-query',init,\n",
    "                   '-db','/mnt/disks/storage/nr/nr',\n",
    "                   '-evalue',EVAL,\n",
    "                   '-out',OUT_FOLDER+hitName+'_blastHomologs',\n",
    "                   '-outfmt','6 sseqid evalue length qlen qstart qend slen sstart send',\n",
    "                  '-max_target_seqs','10000000',\n",
    "                  '-num_threads','6',\n",
    "                  '-taxidlist','bacterial.ids',\n",
    "                  '-qcov_hsp_perc','80']\n",
    "        subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homologs = glob.glob(OUT_FOLDER+\"*_blastHomologs\")\n",
    "homologs2main = dict() #To map protein accessions onto original names/IDs\n",
    "protIDs = set() #All homolog accessions\n",
    "for log in homologs:\n",
    "    main = ('_').join(log.split('_')[:-1]) #remove _blastHomolog part of globbed name\n",
    "    IDs = pd.read_csv(log,delim_whitespace = True,usecols=[0],header=None,squeeze = True).tolist() #read in all IDs\n",
    "    IDs = [x.split(\"|\")[1] for x in IDs] #remove genbank or refseq demarkation attached to accession by | mark\n",
    "    homologs2main = {**homologs2main,**{key:main for key in IDs}} #Add accessions mapped to main names into dictionary\n",
    "    protIDs.update(set(IDs)) #all all IDs into set\n",
    "protIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiprocessing \n",
    "#Search through feature tables to see if protein homologs are contained within those files\n",
    "def CheckForHomologs(FT):\n",
    "    FeatTable = pd.read_csv(FT,sep = '\\t',usecols = ['product_accession','non-redundant_refseq'])\n",
    "    command = ['printf','Checking table '+FT]\n",
    "    subprocess.run(command)\n",
    "    if (any(ID in protIDs for ID in FeatTable['product_accession'])) or (any(ID in protIDs for ID in FeatTable['non-redundant_refseq'])):\n",
    "        prod = [ID for ID in FeatTable['product_accession'] if ID in protIDs]\n",
    "        ref = [ID for ID in FeatTable['non-redundant_refseq'] if ID in protIDs]\n",
    "        Accs = list(set(prod+ref))\n",
    "        return (FT,Accs)\n",
    "    else:\n",
    "        return (FT,[])\n",
    "if __name__ == '__main__':\n",
    "    FTs = glob.glob('/mnt/disks/storage/ncbi-genomes-2021-04-29/*_feature_table.txt')\n",
    "    with Pool() as pool:\n",
    "        haveHomologs = pool.map(CheckForHomologs,FTs)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haveHomolog = pd.DataFrame(haveHomologs,columns = ['Files','ID Present'])\n",
    "haveHomolog.to_csv(OUT_FOLDER+'HomologsinFiles_TaxControls.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haveHomolog = pd.read_csv(OUT_FOLDER+'HomologsinFiles_TaxControls.txt',sep = '\\t',index_col = 0,converters={'ID Present': pd.eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haveHomolog = haveHomolog.where(haveHomolog['ID Present'].str.len() != 0).dropna()\n",
    "haveHomolog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SingleGeneSys = ['ToxN']\n",
    "for sys in SingleGeneSys:\n",
    "    OUT = OUT_FOLDER+'TaxControlSummaries/'+sys+'_TaxIDs'\n",
    "    fileFound = []\n",
    "    homologs = fetchHomologList(sys)\n",
    "    for index, row in haveHomolog.iterrows():\n",
    "        if any([acc in homologs for acc in row['ID Present']]):\n",
    "            fileFound.append(row['Files'])\n",
    "    outDF = pd.DataFrame([None]*len(fileFound),columns = ['Accession'])\n",
    "    outDF['Taxid'] = None\n",
    "    outDF['Species Taxid'] = None\n",
    "    for index, row in outDF.iterrows():\n",
    "        file = fileFound[index]\n",
    "        Acc = os.path.basename(file.split('.')[0])\n",
    "        version = file.split('.')[1].split('_')[0]\n",
    "        Acc = Acc + '.' + version\n",
    "        outDF.at[index,'Accession'] = Acc\n",
    "        outDF.at[index,'Taxid'] = Acc2Taxid[Acc][0]\n",
    "        outDF.at[index,'Species Taxid'] = Acc2Taxid[Acc][1]\n",
    "    outDF.to_csv(OUT,sep = '\\t',index_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sys in MultiGeneSys:\n",
    "    OUT = OUT_FOLDER+'TaxControlSummaries/'+sys+'_TaxIDs'\n",
    "    if os.path.isfile(OUT):\n",
    "        continue\n",
    "    fileFound = []\n",
    "    parts = [os.path.basename(part[:-1]) for part in glob.glob(OUT_FOLDER+sys+'*/')]\n",
    "    initHomologs = fetchHomologList(parts[0])\n",
    "    otherHomologs = [fetchHomologList(ID) for ID in parts[1:]]\n",
    "    for index, row in haveHomolog.iterrows():\n",
    "        if any([ID in initHomologs for ID in row['ID Present']]):\n",
    "            FeatTable = pd.read_csv(row['Files'],sep = '\\t')\n",
    "            FeatTable = FeatTable[FeatTable['# feature'] == 'CDS']\n",
    "            prots = FeatTable[(FeatTable['product_accession'].isin(initHomologs)) | (FeatTable['non-redundant_refseq'].isin(initHomologs))]\n",
    "            \n",
    "            for indexFT, rowFT in prots.iterrows():\n",
    "\n",
    "                if str(rowFT['chromosome']) == 'nan':\n",
    "                    contig = rowFT['genomic_accession']\n",
    "                    up10 = rowFT['start'] - 10000\n",
    "                    down10 = rowFT['end'] + 10000\n",
    "                    aroundProt = FeatTable[(FeatTable['start'] > up10) & \n",
    "                                                 (FeatTable['end'] < down10) &\n",
    "                                                (FeatTable['genomic_accession'] == contig)]\n",
    "                else:\n",
    "                    chromosome = rowFT['chromosome']\n",
    "                    contig = rowFT['genomic_accession']\n",
    "                    up10 = rowFT['start'] - 10000\n",
    "                    down10 = rowFT['end'] + 10000\n",
    "                    aroundProt = FeatTable[(FeatTable['start'] > up10) & \n",
    "                                                 (FeatTable['end'] < down10) &\n",
    "                                                (FeatTable['chromosome'] == chromosome) &\n",
    "                                                (FeatTable['genomic_accession'] == contig)]\n",
    "                allPartsPresent = [False]*len(otherHomologs)\n",
    "                for i in range(len(allPartsPresent)):\n",
    "                    allPartsPresent[i] = any([any([ID in aroundProt['product_accession'].tolist() for ID in otherHomologs[i]]),\n",
    "                                              any([ID in aroundProt['non-redundant_refseq'].tolist() for ID in otherHomologs[i]])])\n",
    "                if all(allPartsPresent):\n",
    "                    fileFound.append(row['Files'])\n",
    "    outDF = pd.DataFrame([None]*len(fileFound),columns = ['Accession'])\n",
    "    outDF['Taxid'] = None\n",
    "    outDF['Species Taxid'] = None\n",
    "    for index, row in outDF.iterrows():\n",
    "        file = fileFound[index]\n",
    "        Acc = os.path.basename(file.split('.')[0])\n",
    "        version = file.split('.')[1].split('_')[0]\n",
    "        Acc = Acc + '.' + version\n",
    "        outDF.at[index,'Accession'] = Acc\n",
    "        outDF.at[index,'Taxid'] = Acc2Taxid[Acc][0]\n",
    "        outDF.at[index,'Species Taxid'] = Acc2Taxid[Acc][1]\n",
    "    outDF.to_csv(OUT,sep = '\\t',index_label = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
